{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2a4891-c257-4d6b-afb3-e8fef39d0437",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f678e62-7bcb-4405-86ae-dce94f494303",
   "metadata": {},
   "source": [
    "# The Main Data Loading Pipeline Summarized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070000fc-a7b7-4c56-a2c0-a938d413a790",
   "metadata": {},
   "source": [
    "The complete chapter code is located in [ch02.ipynb](./ch02.ipynb).\n",
    "\n",
    "This notebook contains the main takeaway, the data loading pipeline without the intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e8f2d-cb81-41a3-8780-a70b382e18ae",
   "metadata": {},
   "source": [
    "Packages that are being used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ed6fbe-45ac-40ce-8ea5-4edb212565e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.8.0\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "# NBVAL_SKIP\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ed4b7db-3b47-4fd3-a4a6-5f4ed5dd166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the book: 5145\n",
      "First 10 token ids: [40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138]\n",
      "Total sequences created: 1286\n",
      "First input sequence (token ids): tensor([  40,  367, 2885, 1464])\n",
      "First target sequence (token ids): tensor([ 367, 2885, 1464, 1807])\n",
      "Secondend input sequence (token ids): tensor([1807, 3619,  402,  271])\n",
      "Secondend target sequence (token ids): tensor([ 3619,   402,   271, 10899])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Transfer Input Text into Tokenized Text into Token Ids\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        print(f\"Total tokens in the book: {len(token_ids)}\")\n",
    "        print(f\"First 10 token ids: {token_ids[:10]}\")\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "        print(f\"Total sequences created: {len(self.input_ids)}\")\n",
    "        print(f\"First input sequence (token ids): {self.input_ids[0]}\")\n",
    "        print(f\"First target sequence (token ids): {self.target_ids[0]}\")\n",
    "        print(f\"Secondend input sequence (token ids): {self.input_ids[1]}\")\n",
    "        print(f\"Secondend target sequence (token ids): {self.target_ids[1]}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size, max_length, stride,\n",
    "                         shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "batch_size = 8\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length,\n",
    "    stride=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664397bc-6daa-4b88-90aa-e8fc1fbd5846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.type of Embedding(50257, 256)>\n",
      "<bound method Module.type of Embedding(1024, 256)>\n",
      "Input batch shape (token ids): torch.Size([8, 4])\n",
      "Target batch shape (token ids): torch.Size([8, 4])\n",
      "Token embeddings shape: torch.Size([8, 4, 256])\n",
      "Token embeddings: tensor([[[-0.7671, -0.2551,  2.1522,  ..., -0.8214, -1.0617,  1.5718],\n",
      "         [-0.2754,  0.5838, -0.0417,  ..., -1.0125,  0.8731,  0.2547],\n",
      "         [ 0.4039, -0.3161, -0.0123,  ..., -0.5068,  0.6774,  0.2407],\n",
      "         [-0.2328,  0.9488, -1.3629,  ...,  0.6063,  1.1937,  0.9867]]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Position embeddings shape: torch.Size([4, 256])\n",
      "Position embeddings: tensor([[ 1.6189,  0.5117,  0.4124,  0.1558,  0.4487,  0.9925, -1.8128, -0.0837,\n",
      "          0.8679, -2.0128, -0.1819,  0.5260, -1.2032,  0.8039, -1.3730, -0.0836,\n",
      "         -0.9995, -0.6364,  0.6700,  1.3646, -0.1285,  0.6210, -2.2872, -1.1659,\n",
      "          0.0679, -1.1508, -1.1403, -0.2682, -0.9396, -1.7957,  0.4869, -0.1173,\n",
      "          0.8913,  1.7132, -1.1887, -1.7801, -0.4220,  0.2360,  0.0114, -0.6582,\n",
      "          0.7815,  1.6159, -1.0157, -0.2223, -0.1717,  1.2029, -0.6460,  0.2101,\n",
      "          0.4339, -1.7084, -1.2372,  2.3976,  0.9621, -0.0062, -0.1130,  0.9761,\n",
      "          0.7681, -0.3490,  1.1314,  0.9824,  0.4560,  0.1138, -0.5178,  0.4743,\n",
      "          0.8478,  1.1249, -0.7162, -0.2776,  1.5919, -2.2063,  0.7269,  1.1711,\n",
      "          0.1552,  1.4669, -0.5109,  1.8515,  0.0903,  0.5513, -0.0669,  0.2338,\n",
      "          1.7073, -0.6827,  1.9620, -0.2626, -2.1876,  0.1637,  0.2457, -0.8886,\n",
      "          0.3300,  1.3207,  0.8347, -0.1038,  0.7656, -0.5051, -0.7375, -1.1192,\n",
      "         -1.0941, -2.5444, -0.8301, -1.4164,  0.9516, -0.0308,  0.0733,  0.1375,\n",
      "          1.8808, -1.4841, -1.0251, -0.6156, -2.2702, -0.1837,  0.4374, -0.2196,\n",
      "          0.9011, -0.7958, -0.3044, -0.5519,  0.7137,  0.8760,  2.1814, -0.6211,\n",
      "         -0.6648,  0.1792,  0.0568, -0.9324,  1.0842,  0.1260,  1.4873,  0.2644,\n",
      "          0.9927, -1.1747, -0.7644, -1.5277, -0.3780,  1.7733, -1.1272, -1.0297,\n",
      "         -0.0364, -0.8468,  0.5817,  0.7603,  0.6536,  1.2410,  0.7352, -0.9926,\n",
      "          1.1807,  0.5056, -0.4065, -0.9595, -0.4886,  0.4040,  0.1981,  2.0559,\n",
      "          0.2130, -1.3628, -0.7072,  1.3401,  0.4321,  0.6681,  0.5185,  0.4635,\n",
      "         -2.1109,  1.7820, -0.3350,  1.3831, -0.5676, -0.2030, -1.8538, -1.6529,\n",
      "          0.8545, -0.0085,  0.0664, -0.7314,  1.0425, -1.1010,  0.5056,  0.7533,\n",
      "         -0.9959, -0.1674, -0.5162, -0.3759, -0.2280, -0.0921,  0.6666,  1.2909,\n",
      "          0.4399,  0.1359,  0.1586,  0.1438, -2.3374,  0.5432,  1.0960, -0.3642,\n",
      "         -0.0925,  0.8123, -1.0988, -0.9728,  0.9149, -2.4632,  0.0224,  0.3717,\n",
      "         -1.0303,  0.5553,  1.1622,  2.1609, -1.4348, -0.4919,  0.3600,  1.1896,\n",
      "          1.0209, -1.3603,  0.8458,  0.3900,  0.2399,  0.8618, -0.3438,  0.1360,\n",
      "         -0.8654,  0.0210,  0.5549,  0.4823, -2.3501,  0.2422, -0.3540,  0.4333,\n",
      "         -0.9672, -0.9348, -0.0521, -1.5627,  0.4092, -0.3058,  0.3828,  0.7433,\n",
      "         -0.4529, -0.1830,  0.0802,  0.2160,  0.1828, -0.4535,  1.2450, -2.1244,\n",
      "         -0.3103,  0.2514, -0.1467, -1.6230,  0.1403,  0.3398, -0.4050,  0.1806,\n",
      "         -1.2314,  0.5884,  1.0771,  2.0329, -0.4842, -0.9537, -1.3178,  0.4011]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Adding token embeddings and position embeddings...\n",
      "Input embeddings shape: torch.Size([8, 4, 256])\n",
      "Input embeddings: tensor([[[ 0.8518,  0.2566,  2.5646,  ..., -1.7751, -2.3795,  1.9729],\n",
      "         [-1.1862, -0.9958,  0.4926,  ..., -2.9883,  1.9190,  1.0408],\n",
      "         [ 0.2789,  0.4739,  0.6316,  ...,  1.4568,  0.0545, -0.9129],\n",
      "         [ 0.4049,  1.8884, -1.8826,  ...,  1.2592,  1.7238,  1.7457]],\n",
      "\n",
      "        [[-0.0986,  0.4140, -0.7885,  ...,  0.6578, -0.2183,  2.1903],\n",
      "         [-0.6262, -1.9902, -0.5360,  ..., -1.3946,  1.8346,  1.4065],\n",
      "         [ 0.4508,  0.8141,  2.5548,  ...,  0.3540,  0.0755, -2.8733],\n",
      "         [ 1.9901,  1.5804, -1.0056,  ...,  0.9313, -0.4264, -1.2417]],\n",
      "\n",
      "        [[ 3.4592,  0.5024, -1.3739,  ...,  0.3482,  0.4675,  0.8928],\n",
      "         [ 1.4252, -1.9760, -0.0234,  ..., -0.9433, -0.7771, -0.6286],\n",
      "         [-1.3580,  1.1915, -0.1460,  ...,  2.4124, -1.1510, -0.8359],\n",
      "         [-1.6832,  0.9006, -0.0899,  ...,  0.5629,  1.7047,  1.9746]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4216,  1.1256,  0.8362,  ..., -0.1147, -1.7177,  0.1370],\n",
      "         [ 0.2941, -3.3914, -0.7434,  ..., -2.1626, -0.3923, -0.0690],\n",
      "         [-0.5804,  0.5485, -0.2764,  ...,  4.1719,  1.3620, -2.1257],\n",
      "         [ 1.3314,  2.1975, -2.0602,  ...,  2.0447,  1.3776, -0.2292]],\n",
      "\n",
      "        [[-0.2370,  0.6279, -0.6771,  ..., -2.3922, -1.2825,  1.5401],\n",
      "         [-0.8315, -1.5545, -1.0844,  ..., -1.9898,  1.0165,  0.6416],\n",
      "         [ 0.8843,  2.5566, -1.4479,  ...,  3.2968, -3.1578,  0.7981],\n",
      "         [-1.1039,  2.9698, -0.3685,  ..., -0.1331, -0.4077, -0.5716]],\n",
      "\n",
      "        [[ 2.8307, -0.5205,  0.3781,  ..., -0.4670, -1.1816,  0.1252],\n",
      "         [-1.9364, -2.4581, -0.9292,  ..., -1.3399,  1.3643,  1.0153],\n",
      "         [-0.8921,  0.5349,  2.7961,  ...,  1.1422, -1.6847,  0.4182],\n",
      "         [ 0.8077,  1.5686,  0.3075,  ...,  0.3017,  1.2505,  0.6375]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257 # Total tokens in the book: 5145 \n",
    "output_dim = 256\n",
    "context_length = 1024\n",
    "\n",
    "# Token Id -> Token Embedding\n",
    "# 50257 x 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(token_embedding_layer.type)\n",
    "\n",
    "# Position Id -> Position Embedding\n",
    "# 1024 x 256\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "print(pos_embedding_layer.type)\n",
    "\n",
    "\n",
    "for batch in dataloader:\n",
    "    # 8 x 4 \n",
    "    inputs, targets = batch\n",
    "    print(\"Input batch shape (token ids):\", inputs.shape)\n",
    "    print(\"Target batch shape (token ids):\", targets.shape)\n",
    "\n",
    "    # 8 x 4 x 256\n",
    "    token_embeddings = token_embedding_layer(inputs)\n",
    "    print(\"Token embeddings shape:\", token_embeddings.shape)\n",
    "    print(\"Token embeddings:\", token_embeddings[:1])  # Print the first 1 token embeddings for brevity\n",
    "\n",
    "    # 4 x 256\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "    print(\"Position embeddings shape:\", pos_embeddings.shape)\n",
    "    print(\"Position embeddings:\", pos_embeddings[:1])  # Print the first 1 position embeddings for brevity\n",
    "\n",
    "    # 8 x 4 x 256 + 4 x 256 --> 8 x 4 x 256\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "    print(\"Input embeddings shape:\", input_embeddings.shape)\n",
    "    print(\"Input embeddings:\", input_embeddings)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
